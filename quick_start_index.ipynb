{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/hudi/\n",
      "\n",
      "0 directories, 0 files\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">import </span></span><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">sys.process._</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">res1_1</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">res1_2</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result3</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\u001b[39m\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\u001b[39m\n",
       "\u001b[36mres1_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\u001b[39m\n",
       "\u001b[36mresult3\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"rm -rf /tmp/hudi/\".!!\n",
    "\"mkdir -p /tmp/hudi\".!!\n",
    "lazy val result3 = \"tree -a /tmp/hudi/\".!!\n",
    "println(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/02 22:55:50 WARN Utils: Your hostname, DESKTOP-M94RUSC resolves to a loopback address: 127.0.1.1; using 172.17.75.227 instead (on interface eth0)\n",
      "25/03/02 22:55:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/02 22:55:51 INFO SparkContext: Running Spark version 3.3.2\n",
      "25/03/02 22:55:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/02 22:55:51 INFO ResourceUtils: ==============================================================\n",
      "25/03/02 22:55:52 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/02 22:55:52 INFO ResourceUtils: ==============================================================\n",
      "25/03/02 22:55:52 INFO SparkContext: Submitted application: HudiLocalSession\n",
      "25/03/02 22:55:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/02 22:55:52 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/03/02 22:55:52 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/02 22:55:52 INFO SecurityManager: Changing view acls to: ryan\n",
      "25/03/02 22:55:52 INFO SecurityManager: Changing modify acls to: ryan\n",
      "25/03/02 22:55:52 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/02 22:55:52 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/02 22:55:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ryan); groups with view permissions: Set(); users  with modify permissions: Set(ryan); groups with modify permissions: Set()\n",
      "25/03/02 22:55:52 INFO Utils: Successfully started service 'sparkDriver' on port 41991.\n",
      "25/03/02 22:55:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/02 22:55:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/02 22:55:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/02 22:55:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/02 22:55:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/02 22:55:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-55d6c695-de56-40ac-8d98-9031bfbba12e\n",
      "25/03/02 22:55:52 INFO MemoryStore: MemoryStore started with capacity 4.4 GiB\n",
      "25/03/02 22:55:52 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/02 22:55:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/02 22:55:52 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/03/02 22:55:52 INFO Executor: Starting executor ID driver on host 172.17.75.227\n",
      "25/03/02 22:55:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/03/02 22:55:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43431.\n",
      "25/03/02 22:55:52 INFO NettyBlockTransferService: Server created on 172.17.75.227:43431\n",
      "25/03/02 22:55:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/02 22:55:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.75.227, 43431, None)\n",
      "25/03/02 22:55:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.75.227:43431 with 4.4 GiB RAM, BlockManagerId(driver, 172.17.75.227, 43431, None)\n",
      "25/03/02 22:55:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.75.227, 43431, None)\n",
      "25/03/02 22:55:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.17.75.227, 43431, None)\n",
      "Spark with Hudi is ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$cp.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConversions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.DataSourceReadOptions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.DataSourceWriteOptions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.common.table.HoodieTableConfig._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.config.HoodieWriteConfig._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.keygen.constant.KeyGeneratorOptions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.common.model.HoodieRecord\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@479042c7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.hudi:hudi-spark3.3-bundle_2.12:1.0.1`\n",
    "// import $ivy.`org.apache.hudi:hudi-common:1.0.0`\n",
    "import $ivy.`org.apache.spark:spark-sql_2.12:3.3.2`\n",
    "import $ivy.`org.apache.spark:spark-hive_2.12:3.3.2`\n",
    "import $cp.`hudi/CustomMergeIntoConnector.jar`\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.hudi.DataSourceReadOptions._\n",
    "import org.apache.hudi.DataSourceWriteOptions._\n",
    "import org.apache.hudi.common.table.HoodieTableConfig._\n",
    "import org.apache.hudi.config.HoodieWriteConfig._\n",
    "import org.apache.hudi.keygen.constant.KeyGeneratorOptions._\n",
    "import org.apache.hudi.common.model.HoodieRecord\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"HudiLocalSession\")\n",
    "  .master(\"local[*]\")  // Runs on local machine\n",
    "  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "  .config(\"spark.kryo.registrator\", \"org.apache.spark.HoodieKryoRegistrar\")\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "  .config(\"spark.kryo.registrator\", \"org.apache.spark.HoodieSparkKryoRegistrar\")\n",
    "  .config(\"spark.driver.extraJavaOptions\", \"-Dscala.repl.maxprintstring=0\")\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "  \n",
    "// http://localhost:4040/jobs/\n",
    "println(\"Spark with Hudi is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cell3.sc:21: value RECORDKEY_FIELD_OPT_KEY in class KeyGeneratorOptions is deprecated\n",
      "  option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n",
      "         ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n",
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">import </span></span><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">spark.implicits._</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">tableName</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;trips_table_index&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">basePath</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;file:///tmp/hudi/hudi_indexed_table&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">columns</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Seq</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>] = <span style=\"color: yellow\"><span class=\"ansi-yellow-fg\">List</span></span>(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;ts&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;uuid&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;fare&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;city&quot;</span></span>)\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">data</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Seq</span></span>[(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Long</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>)] = <span style=\"color: yellow\"><span class=\"ansi-yellow-fg\">List</span></span>(\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695159649087L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;334e26e9-8355-45cc-97c6-c31daf0df330&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-A&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-K&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">19.1</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;san_francisco&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695091554788L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;e96c4396-3fad-413a-a942-4cb36106d721&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-C&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-M&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">27.7</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;san_francisco&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695046462179L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;9909a8b1-2d15-4d3d-8ec9-efc48c536a00&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-D&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-L&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">33.9</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;san_francisco&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695516137016L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;e3cf430c-889d-4015-bc98-59bdce1e530c&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-F&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-P&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">34.15</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;sao_paulo&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695115999911L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;c8abbe79-8d89-47ea-b4ce-4d224bae5bfa&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-J&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-T&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">17.85</span></span>,\n",
       "...\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">inserts</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [ts: bigint, uuid: string ... 4 more fields]</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mtableName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"trips_table_index\"\u001b[39m\n",
       "\u001b[36mbasePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"file:///tmp/hudi/hudi_indexed_table\"\u001b[39m\n",
       "\u001b[36mcolumns\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"ts\"\u001b[39m, \u001b[32m\"uuid\"\u001b[39m, \u001b[32m\"rider\"\u001b[39m, \u001b[32m\"driver\"\u001b[39m, \u001b[32m\"fare\"\u001b[39m, \u001b[32m\"city\"\u001b[39m)\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\n",
       "    \u001b[32m1695159649087L\u001b[39m,\n",
       "    \u001b[32m\"334e26e9-8355-45cc-97c6-c31daf0df330\"\u001b[39m,\n",
       "    \u001b[32m\"rider-A\"\u001b[39m,\n",
       "    \u001b[32m\"driver-K\"\u001b[39m,\n",
       "    \u001b[32m19.1\u001b[39m,\n",
       "    \u001b[32m\"san_francisco\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695091554788L\u001b[39m,\n",
       "    \u001b[32m\"e96c4396-3fad-413a-a942-4cb36106d721\"\u001b[39m,\n",
       "    \u001b[32m\"rider-C\"\u001b[39m,\n",
       "    \u001b[32m\"driver-M\"\u001b[39m,\n",
       "    \u001b[32m27.7\u001b[39m,\n",
       "    \u001b[32m\"san_francisco\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695046462179L\u001b[39m,\n",
       "    \u001b[32m\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\"\u001b[39m,\n",
       "    \u001b[32m\"rider-D\"\u001b[39m,\n",
       "    \u001b[32m\"driver-L\"\u001b[39m,\n",
       "    \u001b[32m33.9\u001b[39m,\n",
       "    \u001b[32m\"san_francisco\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695516137016L\u001b[39m,\n",
       "    \u001b[32m\"e3cf430c-889d-4015-bc98-59bdce1e530c\"\u001b[39m,\n",
       "    \u001b[32m\"rider-F\"\u001b[39m,\n",
       "    \u001b[32m\"driver-P\"\u001b[39m,\n",
       "    \u001b[32m34.15\u001b[39m,\n",
       "    \u001b[32m\"sao_paulo\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695115999911L\u001b[39m,\n",
       "    \u001b[32m\"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa\"\u001b[39m,\n",
       "    \u001b[32m\"rider-J\"\u001b[39m,\n",
       "    \u001b[32m\"driver-T\"\u001b[39m,\n",
       "    \u001b[32m17.85\u001b[39m,\n",
       "...\n",
       "\u001b[36minserts\u001b[39m: \u001b[32mDataFrame\u001b[39m = [ts: bigint, uuid: string ... 4 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val tableName = \"trips_table_index\"\n",
    "val basePath = \"file:///tmp/hudi/hudi_indexed_table\"\n",
    "\n",
    "val columns = Seq(\"ts\",\"uuid\",\"rider\",\"driver\",\"fare\",\"city\")\n",
    "val data =\n",
    "  Seq((1695159649087L,\"334e26e9-8355-45cc-97c6-c31daf0df330\",\"rider-A\",\"driver-K\",19.10,\"san_francisco\"),\n",
    "    (1695091554788L,\"e96c4396-3fad-413a-a942-4cb36106d721\",\"rider-C\",\"driver-M\",27.70 ,\"san_francisco\"),\n",
    "    (1695046462179L,\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\",\"rider-D\",\"driver-L\",33.90 ,\"san_francisco\"),\n",
    "    (1695516137016L,\"e3cf430c-889d-4015-bc98-59bdce1e530c\",\"rider-F\",\"driver-P\",34.15,\"sao_paulo\"    ),\n",
    "    (1695115999911L,\"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa\",\"rider-J\",\"driver-T\",17.85,\"chennai\"));\n",
    "\n",
    "var inserts = spark.createDataFrame(data).toDF(columns:_*)\n",
    "inserts.write.format(\"hudi\").\n",
    "  option(\"hoodie.datasource.write.partitionpath.field\", \"city\").\n",
    "  option(\"hoodie.table.name\", tableName).\n",
    "  option(\"hoodie.write.record.merge.mode\", \"COMMIT_TIME_ORDERING\").\n",
    "  option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n",
    "  mode(Overwrite).\n",
    "  save(basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice that we have 1 extra file instead of 1 file reduction.\n",
    "* This is called as tombstoning.\n",
    "* Instead of deleting right away, we are creating a delete flag for the target rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/hudi/hudi_indexed_table\n",
      "├── .hoodie\n",
      "│   ├── .aux\n",
      "│   │   └── .bootstrap\n",
      "│   │       ├── .fileids\n",
      "│   │       └── .partitions\n",
      "│   ├── .hoodie.properties.crc\n",
      "│   ├── .index_defs\n",
      "│   │   ├── .index.json.crc\n",
      "│   │   └── index.json\n",
      "│   ├── .schema\n",
      "│   ├── .temp\n",
      "│   ├── hoodie.properties\n",
      "│   ├── metadata\n",
      "│   │   ├── .hoodie\n",
      "│   │   │   ├── .aux\n",
      "│   │   │   │   └── .bootstrap\n",
      "│   │   │   │       ├── .fileids\n",
      "│   │   │   │       └── .partitions\n",
      "│   │   │   ├── .hoodie.properties.crc\n",
      "│   │   │   ├── .schema\n",
      "│   │   │   ├── .temp\n",
      "│   │   │   ├── hoodie.properties\n",
      "│   │   │   └── timeline\n",
      "│   │   │       ├── .00000000000000000.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000000.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000000_20250302225556129.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000001_20250302225556334.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000002_20250302225556733.deltacommit.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225554839_20250302225558306.deltacommit.crc\n",
      "│   │   │       ├── 00000000000000000.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000000.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000000_20250302225556129.deltacommit\n",
      "│   │   │       ├── 00000000000000001.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000001.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000001_20250302225556334.deltacommit\n",
      "│   │   │       ├── 00000000000000002.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000002.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000002_20250302225556733.deltacommit\n",
      "│   │   │       ├── 20250302225554839.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225554839.deltacommit.requested\n",
      "│   │   │       ├── 20250302225554839_20250302225558306.deltacommit\n",
      "│   │   │       └── history\n",
      "│   │   ├── column_stats\n",
      "│   │   │   ├── ..col-stats-0000-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0000-0_20250302225554839.log.1_0-42-85.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_20250302225554839.log.1_1-42-86.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .col-stats-0000-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0000-0_20250302225554839.log.1_0-42-85\n",
      "│   │   │   ├── .col-stats-0001-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0001-0_20250302225554839.log.1_1-42-86\n",
      "│   │   │   └── .hoodie_partition_metadata\n",
      "│   │   ├── files\n",
      "│   │   │   ├── ..files-0000-0_00000000000000000.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..files-0000-0_20250302225554839.log.1_3-42-88.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .files-0000-0_0-5-4_00000000000000000.hfile.crc\n",
      "│   │   │   ├── .files-0000-0_00000000000000000.log.1_0-0-0\n",
      "│   │   │   ├── .files-0000-0_20250302225554839.log.1_3-42-88\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   └── files-0000-0_0-5-4_00000000000000000.hfile\n",
      "│   │   └── partition_stats\n",
      "│   │       ├── ..hoodie_partition_metadata.crc\n",
      "│   │       ├── ..partition-stats-0000-0_00000000000000002.log.1_0-0-0.crc\n",
      "│   │       ├── ..partition-stats-0000-0_20250302225554839.log.1_2-42-87.crc\n",
      "│   │       ├── .hoodie_partition_metadata\n",
      "│   │       ├── .partition-stats-0000-0_00000000000000002.log.1_0-0-0\n",
      "│   │       └── .partition-stats-0000-0_20250302225554839.log.1_2-42-87\n",
      "│   └── timeline\n",
      "│       ├── .20250302225554839.commit.requested.crc\n",
      "│       ├── .20250302225554839.inflight.crc\n",
      "│       ├── .20250302225554839_20250302225558333.commit.crc\n",
      "│       ├── 20250302225554839.commit.requested\n",
      "│       ├── 20250302225554839.inflight\n",
      "│       ├── 20250302225554839_20250302225558333.commit\n",
      "│       └── history\n",
      "├── chennai\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   └── 56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet\n",
      "├── san_francisco\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   └── 057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet\n",
      "└── sao_paulo\n",
      "    ├── ..hoodie_partition_metadata.crc\n",
      "    ├── .e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet.crc\n",
      "    ├── .hoodie_partition_metadata\n",
      "    └── e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet\n",
      "\n",
      "26 directories, 72 files\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val result = \"tree -a /tmp/hudi/hudi_indexed_table\".!!\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+\n",
      "|          index_name|       col_name|index_type|\n",
      "+--------------------+---------------+----------+\n",
      "|        column_stats|   column_stats|          |\n",
      "|     partition_stats|partition_stats|          |\n",
      "|expr_index_idx_bl...|  bloom_filters|    driver|\n",
      "+--------------------+---------------+----------+\n",
      "\n",
      "+----+-----+\n",
      "|uuid|rider|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres5_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create record index and secondary index for the table\n",
    "spark.sql(s\"CREATE TABLE IF NOT EXISTS hudi_indexed_table USING hudi LOCATION '$basePath'\")\n",
    "// Create bloom filter expression index on driver column\n",
    "spark.sql(s\"CREATE INDEX idx_bloom_driver ON hudi_indexed_table USING bloom_filters(driver) OPTIONS(expr='identity')\");\n",
    "// It would show bloom filter expression index \n",
    "spark.sql(s\"SHOW INDEXES FROM hudi_indexed_table\").show();\n",
    "// Query on driver column would prune the data using the idx_bloom_driver index\n",
    "spark.sql(s\"SELECT uuid, rider FROM hudi_indexed_table WHERE driver = 'driver-S'\").show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No driver-S, so bloom filter should filter out the query fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the merge mode is COMMIT_TIME_ORDERING, which means: \n",
    "    * (1) COMMIT_TIME_ORDERING: use commit time to merge records, i.e., the record from later commit overwrites the earlier record with the same key. \n",
    "    * https://hudi.apache.org/docs/record_merger/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/hudi/hudi_indexed_table\n",
      "├── .hoodie\n",
      "│   ├── .aux\n",
      "│   │   └── .bootstrap\n",
      "│   │       ├── .fileids\n",
      "│   │       └── .partitions\n",
      "│   ├── .hoodie.properties.crc\n",
      "│   ├── .index_defs\n",
      "│   │   ├── .index.json.crc\n",
      "│   │   └── index.json\n",
      "│   ├── .schema\n",
      "│   ├── .temp\n",
      "│   ├── hoodie.properties\n",
      "│   ├── metadata\n",
      "│   │   ├── .hoodie\n",
      "│   │   │   ├── .aux\n",
      "│   │   │   │   └── .bootstrap\n",
      "│   │   │   │       ├── .fileids\n",
      "│   │   │   │       └── .partitions\n",
      "│   │   │   ├── .heartbeat\n",
      "│   │   │   ├── .hoodie.properties.crc\n",
      "│   │   │   ├── .schema\n",
      "│   │   │   ├── .temp\n",
      "│   │   │   ├── hoodie.properties\n",
      "│   │   │   └── timeline\n",
      "│   │   │       ├── .00000000000000000.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000000.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000000_20250302225556129.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000001_20250302225556334.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000002_20250302225556733.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000003.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000003.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000003_20250302225600414.deltacommit.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225554839_20250302225558306.deltacommit.crc\n",
      "│   │   │       ├── 00000000000000000.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000000.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000000_20250302225556129.deltacommit\n",
      "│   │   │       ├── 00000000000000001.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000001.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000001_20250302225556334.deltacommit\n",
      "│   │   │       ├── 00000000000000002.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000002.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000002_20250302225556733.deltacommit\n",
      "│   │   │       ├── 00000000000000003.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000003.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000003_20250302225600414.deltacommit\n",
      "│   │   │       ├── 20250302225554839.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225554839.deltacommit.requested\n",
      "│   │   │       ├── 20250302225554839_20250302225558306.deltacommit\n",
      "│   │   │       └── history\n",
      "│   │   ├── column_stats\n",
      "│   │   │   ├── ..col-stats-0000-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0000-0_20250302225554839.log.1_0-42-85.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_20250302225554839.log.1_1-42-86.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .col-stats-0000-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0000-0_20250302225554839.log.1_0-42-85\n",
      "│   │   │   ├── .col-stats-0001-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0001-0_20250302225554839.log.1_1-42-86\n",
      "│   │   │   └── .hoodie_partition_metadata\n",
      "│   │   ├── expr_index_idx_bloom_driver\n",
      "│   │   │   ├── ..expr-index-idx-bloom-driver-0000-0_00000000000000003.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..expr-index-idx-bloom-driver-0001-0_00000000000000003.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0000-0_0-53-105_00000000000000003.hfile.crc\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0000-0_00000000000000003.log.1_0-0-0\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0001-0_00000000000000003.log.1_0-0-0\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0001-0_1-53-106_00000000000000003.hfile.crc\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   ├── expr-index-idx-bloom-driver-0000-0_0-53-105_00000000000000003.hfile\n",
      "│   │   │   └── expr-index-idx-bloom-driver-0001-0_1-53-106_00000000000000003.hfile\n",
      "│   │   ├── files\n",
      "│   │   │   ├── ..files-0000-0_00000000000000000.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..files-0000-0_20250302225554839.log.1_3-42-88.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .files-0000-0_0-5-4_00000000000000000.hfile.crc\n",
      "│   │   │   ├── .files-0000-0_00000000000000000.log.1_0-0-0\n",
      "│   │   │   ├── .files-0000-0_20250302225554839.log.1_3-42-88\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   └── files-0000-0_0-5-4_00000000000000000.hfile\n",
      "│   │   └── partition_stats\n",
      "│   │       ├── ..hoodie_partition_metadata.crc\n",
      "│   │       ├── ..partition-stats-0000-0_00000000000000002.log.1_0-0-0.crc\n",
      "│   │       ├── ..partition-stats-0000-0_20250302225554839.log.1_2-42-87.crc\n",
      "│   │       ├── .hoodie_partition_metadata\n",
      "│   │       ├── .partition-stats-0000-0_00000000000000002.log.1_0-0-0\n",
      "│   │       └── .partition-stats-0000-0_20250302225554839.log.1_2-42-87\n",
      "│   └── timeline\n",
      "│       ├── .20250302225554839.commit.requested.crc\n",
      "│       ├── .20250302225554839.inflight.crc\n",
      "│       ├── .20250302225554839_20250302225558333.commit.crc\n",
      "│       ├── .20250302225559340.indexing.inflight.crc\n",
      "│       ├── .20250302225559340.indexing.requested.crc\n",
      "│       ├── .20250302225559340_20250302225600498.indexing.crc\n",
      "│       ├── 20250302225554839.commit.requested\n",
      "│       ├── 20250302225554839.inflight\n",
      "│       ├── 20250302225554839_20250302225558333.commit\n",
      "│       ├── 20250302225559340.indexing.inflight\n",
      "│       ├── 20250302225559340.indexing.requested\n",
      "│       ├── 20250302225559340_20250302225600498.indexing\n",
      "│       └── history\n",
      "├── chennai\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   └── 56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet\n",
      "├── san_francisco\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   └── 057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet\n",
      "└── sao_paulo\n",
      "    ├── ..hoodie_partition_metadata.crc\n",
      "    ├── .e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet.crc\n",
      "    ├── .hoodie_partition_metadata\n",
      "    └── e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet\n",
      "\n",
      "28 directories, 94 files\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val result = \"tree -a /tmp/hudi/hudi_indexed_table\".!!\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can find the filter in the metadata dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"total 16\n",
       "drwxr-xr-x 2 ryan ryan 4096 Mar  2 22:55 column_stats\n",
       "drwxr-xr-x 2 ryan ryan 4096 Mar  2 22:56 expr_index_idx_bloom_driver\n",
       "drwxr-xr-x 2 ryan ryan 4096 Mar  2 22:55 files\n",
       "drwxr-xr-x 2 ryan ryan 4096 Mar  2 22:55 partition_stats\n",
       "\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ls -l /tmp/hudi/hudi_indexed_table/.hoodie/metadata\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+-----------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|           ts|                uuid|  rider|  driver| fare|         city|human_readable_ts|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+-----------------+\n",
      "|  20250302225554839|20250302225554839...|334e26e9-8355-45c...|         san_francisco|057fe24d-fb09-452...|1695159649087|334e26e9-8355-45c...|rider-A|driver-K| 19.1|san_francisco|       2023-09-20|\n",
      "|  20250302225554839|20250302225554839...|9909a8b1-2d15-4d3...|         san_francisco|057fe24d-fb09-452...|1695046462179|9909a8b1-2d15-4d3...|rider-D|driver-L| 33.9|san_francisco|       2023-09-18|\n",
      "|  20250302225554839|20250302225554839...|e96c4396-3fad-413...|         san_francisco|057fe24d-fb09-452...|1695091554788|e96c4396-3fad-413...|rider-C|driver-M| 27.7|san_francisco|       2023-09-19|\n",
      "|  20250302225554839|20250302225554839...|e3cf430c-889d-401...|             sao_paulo|e5fab59c-37cd-424...|1695516137016|e3cf430c-889d-401...|rider-F|driver-P|34.15|    sao_paulo|       2023-09-24|\n",
      "|  20250302225554839|20250302225554839...|c8abbe79-8d89-47e...|               chennai|56d56bfc-255f-49d...|1695115999911|c8abbe79-8d89-47e...|rider-J|driver-T|17.85|      chennai|       2023-09-19|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 9 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_1 = spark.read.format(\"hudi\").load(basePath)\n",
    "df_1.withColumn(\"human_readable_ts\", from_unixtime(col(\"ts\")/1000, \"yyyy-MM-dd\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+\n",
      "|          index_name|       col_name|index_type|\n",
      "+--------------------+---------------+----------+\n",
      "|        column_stats|   column_stats|          |\n",
      "|     partition_stats|partition_stats|          |\n",
      "|expr_index_idx_co...|   column_stats|        ts|\n",
      "|expr_index_idx_bl...|  bloom_filters|    driver|\n",
      "+--------------------+---------------+----------+\n",
      "\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+---------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|           ts|                uuid|  rider|  driver| fare|     city|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+---------+\n",
      "|  20250302225554839|20250302225554839...|e3cf430c-889d-401...|             sao_paulo|e5fab59c-37cd-424...|1695516137016|e3cf430c-889d-401...|rider-F|driver-P|34.15|sao_paulo|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create column stat expression index on ts column\n",
    "spark.sql(s\"CREATE INDEX idx_column_ts ON hudi_indexed_table USING column_stats(ts) OPTIONS(expr='from_unixtime', format = 'yyyy-MM-dd')\");\n",
    "// Shows both expression indexes \n",
    "spark.sql(s\"SHOW INDEXES FROM hudi_indexed_table\").show();\n",
    "// Query on ts column would prune the data using the idx_column_ts index\n",
    "spark.sql(s\"SELECT * FROM hudi_indexed_table WHERE from_unixtime(ts/1000, 'yyyy-MM-dd') = '2023-09-24'\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+\n",
      "|          index_name|       col_name|index_type|\n",
      "+--------------------+---------------+----------+\n",
      "|        record_index|   record_index|          |\n",
      "|        column_stats|   column_stats|          |\n",
      "|     partition_stats|partition_stats|          |\n",
      "|secondary_index_i...|secondary_index|     rider|\n",
      "|expr_index_idx_co...|   column_stats|        ts|\n",
      "|expr_index_idx_bl...|  bloom_filters|    driver|\n",
      "+--------------------+---------------+----------+\n",
      "\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name| ts|uuid|rider|driver|fare|city|\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: string, value: string]\n",
       "\u001b[36mres10_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres10_2\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// To create secondary index, first create the record index\n",
    "spark.sql(s\"SET hoodie.metadata.record.index.enable=true\");\n",
    "spark.sql(s\"CREATE INDEX record_index ON hudi_indexed_table (uuid)\");\n",
    "// Create secondary index on rider column\n",
    "spark.sql(s\"CREATE INDEX idx_rider ON hudi_indexed_table (rider)\");\n",
    "\n",
    "// Expression index and secondary index should show up\n",
    "spark.sql(s\"SHOW INDEXES FROM hudi_indexed_table\").show();\n",
    "// Query on rider column would leverage the secondary index idx_rider\n",
    "spark.sql(s\"SELECT * FROM hudi_indexed_table WHERE rider = 'rider-E'\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/hudi/hudi_indexed_table\n",
      "├── .hoodie\n",
      "│   ├── .aux\n",
      "│   │   └── .bootstrap\n",
      "│   │       ├── .fileids\n",
      "│   │       └── .partitions\n",
      "│   ├── .hoodie.properties.crc\n",
      "│   ├── .index_defs\n",
      "│   │   ├── .index.json.crc\n",
      "│   │   └── index.json\n",
      "│   ├── .schema\n",
      "│   ├── .temp\n",
      "│   ├── hoodie.properties\n",
      "│   ├── metadata\n",
      "│   │   ├── .hoodie\n",
      "│   │   │   ├── .aux\n",
      "│   │   │   │   └── .bootstrap\n",
      "│   │   │   │       ├── .fileids\n",
      "│   │   │   │       └── .partitions\n",
      "│   │   │   ├── .heartbeat\n",
      "│   │   │   ├── .hoodie.properties.crc\n",
      "│   │   │   ├── .schema\n",
      "│   │   │   ├── .temp\n",
      "│   │   │   ├── hoodie.properties\n",
      "│   │   │   └── timeline\n",
      "│   │   │       ├── .00000000000000000.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000000.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000000_20250302225556129.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000001_20250302225556334.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000002_20250302225556733.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000003.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000003.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000003_20250302225600414.deltacommit.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225554839_20250302225558306.deltacommit.crc\n",
      "│   │   │       ├── .20250302225559340.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225559340.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225559340_20250302225604091.deltacommit.crc\n",
      "│   │   │       ├── .20250302225603213.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225603213.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225603213_20250302225605321.deltacommit.crc\n",
      "│   │   │       ├── .20250302225604816.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225604816.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225604816_20250302225606067.deltacommit.crc\n",
      "│   │   │       ├── 00000000000000000.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000000.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000000_20250302225556129.deltacommit\n",
      "│   │   │       ├── 00000000000000001.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000001.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000001_20250302225556334.deltacommit\n",
      "│   │   │       ├── 00000000000000002.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000002.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000002_20250302225556733.deltacommit\n",
      "│   │   │       ├── 00000000000000003.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000003.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000003_20250302225600414.deltacommit\n",
      "│   │   │       ├── 20250302225554839.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225554839.deltacommit.requested\n",
      "│   │   │       ├── 20250302225554839_20250302225558306.deltacommit\n",
      "│   │   │       ├── 20250302225559340.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225559340.deltacommit.requested\n",
      "│   │   │       ├── 20250302225559340_20250302225604091.deltacommit\n",
      "│   │   │       ├── 20250302225603213.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225603213.deltacommit.requested\n",
      "│   │   │       ├── 20250302225603213_20250302225605321.deltacommit\n",
      "│   │   │       ├── 20250302225604816.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225604816.deltacommit.requested\n",
      "│   │   │       ├── 20250302225604816_20250302225606067.deltacommit\n",
      "│   │   │       └── history\n",
      "│   │   ├── column_stats\n",
      "│   │   │   ├── ..col-stats-0000-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0000-0_20250302225554839.log.1_0-42-85.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_20250302225554839.log.1_1-42-86.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .col-stats-0000-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0000-0_20250302225554839.log.1_0-42-85\n",
      "│   │   │   ├── .col-stats-0001-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0001-0_20250302225554839.log.1_1-42-86\n",
      "│   │   │   └── .hoodie_partition_metadata\n",
      "│   │   ├── expr_index_idx_bloom_driver\n",
      "│   │   │   ├── ..expr-index-idx-bloom-driver-0000-0_00000000000000003.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..expr-index-idx-bloom-driver-0001-0_00000000000000003.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0000-0_0-53-105_00000000000000003.hfile.crc\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0000-0_00000000000000003.log.1_0-0-0\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0001-0_00000000000000003.log.1_0-0-0\n",
      "│   │   │   ├── .expr-index-idx-bloom-driver-0001-0_1-53-106_00000000000000003.hfile.crc\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   ├── expr-index-idx-bloom-driver-0000-0_0-53-105_00000000000000003.hfile\n",
      "│   │   │   └── expr-index-idx-bloom-driver-0001-0_1-53-106_00000000000000003.hfile\n",
      "│   │   ├── expr_index_idx_column_ts\n",
      "│   │   │   ├── ..expr-index-idx-column-ts-0000-0_20250302225559340.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..expr-index-idx-column-ts-0001-0_20250302225559340.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .expr-index-idx-column-ts-0000-0_0-71-134_20250302225559340.hfile.crc\n",
      "│   │   │   ├── .expr-index-idx-column-ts-0000-0_20250302225559340.log.1_0-0-0\n",
      "│   │   │   ├── .expr-index-idx-column-ts-0001-0_1-71-135_20250302225559340.hfile.crc\n",
      "│   │   │   ├── .expr-index-idx-column-ts-0001-0_20250302225559340.log.1_0-0-0\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   ├── expr-index-idx-column-ts-0000-0_0-71-134_20250302225559340.hfile\n",
      "│   │   │   └── expr-index-idx-column-ts-0001-0_1-71-135_20250302225559340.hfile\n",
      "│   │   ├── files\n",
      "│   │   │   ├── ..files-0000-0_00000000000000000.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..files-0000-0_20250302225554839.log.1_3-42-88.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .files-0000-0_0-5-4_00000000000000000.hfile.crc\n",
      "│   │   │   ├── .files-0000-0_00000000000000000.log.1_0-0-0\n",
      "│   │   │   ├── .files-0000-0_20250302225554839.log.1_3-42-88\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   └── files-0000-0_0-5-4_00000000000000000.hfile\n",
      "│   │   ├── partition_stats\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── ..partition-stats-0000-0_00000000000000002.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..partition-stats-0000-0_20250302225554839.log.1_2-42-87.crc\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   ├── .partition-stats-0000-0_00000000000000002.log.1_0-0-0\n",
      "│   │   │   └── .partition-stats-0000-0_20250302225554839.log.1_2-42-87\n",
      "│   │   ├── record_index\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── ..record-index-0000-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0001-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0002-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0003-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0004-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0005-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0006-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0007-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0008-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..record-index-0009-0_20250302225603213.log.1_0-0-0.crc\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   ├── .record-index-0000-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0001-0_1-82-171_20250302225603213.hfile.crc\n",
      "│   │   │   ├── .record-index-0001-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0002-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0003-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0003-0_3-82-172_20250302225603213.hfile.crc\n",
      "│   │   │   ├── .record-index-0004-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0004-0_4-82-173_20250302225603213.hfile.crc\n",
      "│   │   │   ├── .record-index-0005-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0006-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0007-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0007-0_7-82-174_20250302225603213.hfile.crc\n",
      "│   │   │   ├── .record-index-0008-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0009-0_20250302225603213.log.1_0-0-0\n",
      "│   │   │   ├── .record-index-0009-0_9-82-175_20250302225603213.hfile.crc\n",
      "│   │   │   ├── record-index-0001-0_1-82-171_20250302225603213.hfile\n",
      "│   │   │   ├── record-index-0003-0_3-82-172_20250302225603213.hfile\n",
      "│   │   │   ├── record-index-0004-0_4-82-173_20250302225603213.hfile\n",
      "│   │   │   ├── record-index-0007-0_7-82-174_20250302225603213.hfile\n",
      "│   │   │   └── record-index-0009-0_9-82-175_20250302225603213.hfile\n",
      "│   │   └── secondary_index_idx_rider\n",
      "│   │       ├── ..hoodie_partition_metadata.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0000-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0001-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0002-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0003-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0004-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0005-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0006-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0007-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0008-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── ..secondary-index-idx-rider-0009-0_20250302225604816.log.1_0-0-0.crc\n",
      "│   │       ├── .hoodie_partition_metadata\n",
      "│   │       ├── .secondary-index-idx-rider-0000-0_0-90-209_20250302225604816.hfile.crc\n",
      "│   │       ├── .secondary-index-idx-rider-0000-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0001-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0002-0_2-90-210_20250302225604816.hfile.crc\n",
      "│   │       ├── .secondary-index-idx-rider-0002-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0003-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0004-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0004-0_4-90-211_20250302225604816.hfile.crc\n",
      "│   │       ├── .secondary-index-idx-rider-0005-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0006-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0006-0_6-90-212_20250302225604816.hfile.crc\n",
      "│   │       ├── .secondary-index-idx-rider-0007-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0007-0_7-90-213_20250302225604816.hfile.crc\n",
      "│   │       ├── .secondary-index-idx-rider-0008-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── .secondary-index-idx-rider-0009-0_20250302225604816.log.1_0-0-0\n",
      "│   │       ├── secondary-index-idx-rider-0000-0_0-90-209_20250302225604816.hfile\n",
      "│   │       ├── secondary-index-idx-rider-0002-0_2-90-210_20250302225604816.hfile\n",
      "│   │       ├── secondary-index-idx-rider-0004-0_4-90-211_20250302225604816.hfile\n",
      "│   │       ├── secondary-index-idx-rider-0006-0_6-90-212_20250302225604816.hfile\n",
      "│   │       └── secondary-index-idx-rider-0007-0_7-90-213_20250302225604816.hfile\n",
      "│   └── timeline\n",
      "│       ├── .20250302225554839.commit.requested.crc\n",
      "│       ├── .20250302225554839.inflight.crc\n",
      "│       ├── .20250302225554839_20250302225558333.commit.crc\n",
      "│       ├── .20250302225559340.indexing.inflight.crc\n",
      "│       ├── .20250302225559340.indexing.requested.crc\n",
      "│       ├── .20250302225559340_20250302225600498.indexing.crc\n",
      "│       ├── .20250302225603213.indexing.inflight.crc\n",
      "│       ├── .20250302225603213.indexing.requested.crc\n",
      "│       ├── .20250302225603213_20250302225604159.indexing.crc\n",
      "│       ├── .20250302225604816.indexing.inflight.crc\n",
      "│       ├── .20250302225604816.indexing.requested.crc\n",
      "│       ├── .20250302225604816_20250302225605377.indexing.crc\n",
      "│       ├── .20250302225605606.indexing.inflight.crc\n",
      "│       ├── .20250302225605606.indexing.requested.crc\n",
      "│       ├── .20250302225605606_20250302225606123.indexing.crc\n",
      "│       ├── 20250302225554839.commit.requested\n",
      "│       ├── 20250302225554839.inflight\n",
      "│       ├── 20250302225554839_20250302225558333.commit\n",
      "│       ├── 20250302225559340.indexing.inflight\n",
      "│       ├── 20250302225559340.indexing.requested\n",
      "│       ├── 20250302225559340_20250302225600498.indexing\n",
      "│       ├── 20250302225603213.indexing.inflight\n",
      "│       ├── 20250302225603213.indexing.requested\n",
      "│       ├── 20250302225603213_20250302225604159.indexing\n",
      "│       ├── 20250302225604816.indexing.inflight\n",
      "│       ├── 20250302225604816.indexing.requested\n",
      "│       ├── 20250302225604816_20250302225605377.indexing\n",
      "│       ├── 20250302225605606.indexing.inflight\n",
      "│       ├── 20250302225605606.indexing.requested\n",
      "│       ├── 20250302225605606_20250302225606123.indexing\n",
      "│       └── history\n",
      "├── chennai\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   └── 56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet\n",
      "├── san_francisco\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   └── 057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet\n",
      "└── sao_paulo\n",
      "    ├── ..hoodie_partition_metadata.crc\n",
      "    ├── .e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet.crc\n",
      "    ├── .hoodie_partition_metadata\n",
      "    └── e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet\n",
      "\n",
      "31 directories, 204 files\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val result = \"tree -a /tmp/hudi/hudi_indexed_table\".!!\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/02 22:56:07 ERROR Executor: Exception in task 0.0 in stage 107.0 (TID 264)\n",
      "org.apache.hudi.exception.HoodieException: unable to read next record from parquet file \n",
      "\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:54)\n",
      "\tat org.apache.hudi.common.util.collection.MappingIterator.hasNext(MappingIterator.java:39)\n",
      "\tat java.base/java.util.Iterator.forEachRemaining(Iterator.java:132)\n",
      "\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.getBaseFileRecords(SparkMetadataWriterUtils.java:260)\n",
      "\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.readRecordsAsRows(SparkMetadataWriterUtils.java:232)\n",
      "\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.getExpressionIndexRecordsIterator(SparkMetadataWriterUtils.java:352)\n",
      "\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.lambda$getExprIndexRecords$69dbdb2c$1(SparkMetadataWriterUtils.java:320)\n",
      "\tat org.apache.hudi.data.HoodieJavaRDD.lambda$flatMap$a6598fcb$1(HoodieJavaRDD.java:160)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/hudi/hudi_indexed_table/san_francisco/057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-104-262_20250302225607026.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n",
      "\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:49)\n",
      "\t... 35 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: The requested schema is not compatible with the file schema. incompatible types: required int64 ts != optional int64 ts\n",
      "\tat org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.incompatibleSchema(ColumnIOFactory.java:101)\n",
      "\tat org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visitChildren(ColumnIOFactory.java:81)\n",
      "\tat org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visit(ColumnIOFactory.java:57)\n",
      "\tat org.apache.parquet.schema.MessageType.accept(MessageType.java:55)\n",
      "\tat org.apache.parquet.io.ColumnIOFactory.getColumnIO(ColumnIOFactory.java:162)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:135)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:225)\n",
      "\t... 38 more\n",
      "25/03/02 22:56:07 ERROR TaskSetManager: Task 0 in stage 107.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.hudi.exception.HoodieMetadataException",
     "evalue": "Failed to get expression index updates for partition expr_index_idx_column_ts",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.hudi.exception.HoodieMetadataException: Failed to get expression index updates for partition expr_index_idx_column_ts\u001b[39m",
      "  org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.lambda$updateExpressionIndexIfPresent$29(\u001b[32mHoodieBackedTableMetadataWriter.java\u001b[39m:\u001b[32m1122\u001b[39m)",
      "  java.util.stream.ForEachOps$ForEachOp$OfRef.accept(\u001b[32mForEachOps.java\u001b[39m:\u001b[32m183\u001b[39m)",
      "  java.util.stream.ReferencePipeline$2$1.accept(\u001b[32mReferencePipeline.java\u001b[39m:\u001b[32m177\u001b[39m)",
      "  java.util.HashMap$KeySpliterator.forEachRemaining(\u001b[32mHashMap.java\u001b[39m:\u001b[32m1621\u001b[39m)",
      "  java.util.stream.AbstractPipeline.copyInto(\u001b[32mAbstractPipeline.java\u001b[39m:\u001b[32m484\u001b[39m)",
      "  java.util.stream.AbstractPipeline.wrapAndCopyInto(\u001b[32mAbstractPipeline.java\u001b[39m:\u001b[32m474\u001b[39m)",
      "  java.util.stream.ForEachOps$ForEachOp.evaluateSequential(\u001b[32mForEachOps.java\u001b[39m:\u001b[32m150\u001b[39m)",
      "  java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(\u001b[32mForEachOps.java\u001b[39m:\u001b[32m173\u001b[39m)",
      "  java.util.stream.AbstractPipeline.evaluate(\u001b[32mAbstractPipeline.java\u001b[39m:\u001b[32m234\u001b[39m)",
      "  java.util.stream.ReferencePipeline.forEach(\u001b[32mReferencePipeline.java\u001b[39m:\u001b[32m497\u001b[39m)",
      "  org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.updateExpressionIndexIfPresent(\u001b[32mHoodieBackedTableMetadataWriter.java\u001b[39m:\u001b[32m1117\u001b[39m)",
      "  org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.lambda$update$27(\u001b[32mHoodieBackedTableMetadataWriter.java\u001b[39m:\u001b[32m1099\u001b[39m)",
      "  org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.processAndCommit(\u001b[32mHoodieBackedTableMetadataWriter.java\u001b[39m:\u001b[32m1024\u001b[39m)",
      "  org.apache.hudi.metadata.HoodieBackedTableMetadataWriter.update(\u001b[32mHoodieBackedTableMetadataWriter.java\u001b[39m:\u001b[32m1084\u001b[39m)",
      "  org.apache.hudi.client.BaseHoodieClient.writeTableMetadata(\u001b[32mBaseHoodieClient.java\u001b[39m:\u001b[32m277\u001b[39m)",
      "  org.apache.hudi.client.BaseHoodieWriteClient.commit(\u001b[32mBaseHoodieWriteClient.java\u001b[39m:\u001b[32m286\u001b[39m)",
      "  org.apache.hudi.client.BaseHoodieWriteClient.commitStats(\u001b[32mBaseHoodieWriteClient.java\u001b[39m:\u001b[32m246\u001b[39m)",
      "  org.apache.hudi.client.SparkRDDWriteClient.commit(\u001b[32mSparkRDDWriteClient.java\u001b[39m:\u001b[32m94\u001b[39m)",
      "  org.apache.hudi.HoodieSparkSqlWriterInternal.commitAndPerformPostOperations(\u001b[32mHoodieSparkSqlWriter.scala\u001b[39m:\u001b[32m994\u001b[39m)",
      "  org.apache.hudi.HoodieSparkSqlWriterInternal.writeInternal(\u001b[32mHoodieSparkSqlWriter.scala\u001b[39m:\u001b[32m533\u001b[39m)",
      "  org.apache.hudi.HoodieSparkSqlWriterInternal.$anonfun$write$1(\u001b[32mHoodieSparkSqlWriter.scala\u001b[39m:\u001b[32m192\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m109\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m169\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m95\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m779\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)",
      "  org.apache.spark.sql.adapter.BaseSpark3Adapter.sqlExecutionWithNewExecutionId(\u001b[32mBaseSpark3Adapter.scala\u001b[39m:\u001b[32m105\u001b[39m)",
      "  org.apache.hudi.HoodieSparkSqlWriterInternal.write(\u001b[32mHoodieSparkSqlWriter.scala\u001b[39m:\u001b[32m214\u001b[39m)",
      "  org.apache.hudi.HoodieSparkSqlWriter$.write(\u001b[32mHoodieSparkSqlWriter.scala\u001b[39m:\u001b[32m129\u001b[39m)",
      "  org.apache.hudi.DefaultSource.createRelation(\u001b[32mDefaultSource.scala\u001b[39m:\u001b[32m170\u001b[39m)",
      "  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(\u001b[32mSaveIntoDataSourceCommand.scala\u001b[39m:\u001b[32m47\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m75\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m73\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(\u001b[32mcommands.scala\u001b[39m:\u001b[32m84\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m109\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m169\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m95\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m779\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m94\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m584\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m176\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m584\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m30\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m267\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m263\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m30\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m30\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m560\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m94\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m79\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m116\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m860\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m390\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.saveInternal(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m363\u001b[39m)",
      "  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m247\u001b[39m)",
      "  org.apache.spark.sql.hudi.command.UpdateHoodieTableCommand.run(\u001b[32mUpdateHoodieTableCommand.scala\u001b[39m:\u001b[32m133\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m75\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m73\u001b[39m)",
      "  org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(\u001b[32mcommands.scala\u001b[39m:\u001b[32m84\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m109\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m169\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m95\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m779\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m64\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m98\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m94\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m584\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m176\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m584\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m30\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m267\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m263\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m30\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m30\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m560\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m94\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.commandExecuted(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m79\u001b[39m)",
      "  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m220\u001b[39m)",
      "  org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(\u001b[32mDataset.scala\u001b[39m:\u001b[32m100\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m779\u001b[39m)",
      "  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m97\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.$anonfun$sql$1(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m622\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m779\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.sql(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m617\u001b[39m)",
      "  ammonite.$sess.cell12$Helper.<init>(\u001b[32mcell12.sc\u001b[39m:\u001b[32m3\u001b[39m)",
      "  ammonite.$sess.cell12$.<init>(\u001b[32mcell12.sc\u001b[39m:\u001b[32m7\u001b[39m)",
      "  ammonite.$sess.cell12$.<clinit>(\u001b[32mcell12.sc\u001b[39m:\u001b[32m-1\u001b[39m)",
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 1 times, most recent failure: Lost task 0.0 in stage 107.0 (TID 264) (172.17.75.227 executor driver): org.apache.hudi.exception.HoodieException: unable to read next record from parquet file \n\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:54)\n\tat org.apache.hudi.common.util.collection.MappingIterator.hasNext(MappingIterator.java:39)\n\tat java.base/java.util.Iterator.forEachRemaining(Iterator.java:132)\n\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.getBaseFileRecords(SparkMetadataWriterUtils.java:260)\n\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.readRecordsAsRows(SparkMetadataWriterUtils.java:232)\n\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.getExpressionIndexRecordsIterator(SparkMetadataWriterUtils.java:352)\n\tat org.apache.hudi.client.utils.SparkMetadataWriterUtils.lambda$getExprIndexRecords$69dbdb2c$1(SparkMetadataWriterUtils.java:320)\n\tat org.apache.hudi.data.HoodieJavaRDD.lambda$flatMap$a6598fcb$1(HoodieJavaRDD.java:160)\n\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(JavaRDDLike.scala:125)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/hudi/hudi_indexed_table/san_francisco/057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-104-262_20250302225607026.parquet\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\n\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\n\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\n\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:49)\n\t... 35 more\nCaused by: org.apache.parquet.io.ParquetDecodingException: The requested schema is not compatible with the file schema. incompatible types: required int64 ts != optional int64 ts\n\tat org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.incompatibleSchema(ColumnIOFactory.java:101)\n\tat org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visitChildren(ColumnIOFactory.java:81)\n\tat org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visit(ColumnIOFactory.java:57)\n\tat org.apache.parquet.schema.MessageType.accept(MessageType.java:55)\n\tat org.apache.parquet.io.ColumnIOFactory.getColumnIO(ColumnIOFactory.java:162)\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:135)\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:225)\n\t... 38 more\n\nDriver stacktrace:\u001b[39m",
      "  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2672\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2608\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2607\u001b[39m)",
      "  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)",
      "  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)",
      "  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2607\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1182\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1182\u001b[39m)",
      "  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m407\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1182\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2860\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2802\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2791\u001b[39m)",
      "  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)",
      "\u001b[31morg.apache.hudi.exception.HoodieException: unable to read next record from parquet file \u001b[39m",
      "  org.apache.hudi.common.util.ParquetReaderIterator.hasNext(\u001b[32mParquetReaderIterator.java\u001b[39m:\u001b[32m54\u001b[39m)",
      "  org.apache.hudi.common.util.collection.MappingIterator.hasNext(\u001b[32mMappingIterator.java\u001b[39m:\u001b[32m39\u001b[39m)",
      "  java.util.Iterator.forEachRemaining(\u001b[32mIterator.java\u001b[39m:\u001b[32m132\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.getBaseFileRecords(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m260\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.readRecordsAsRows(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m232\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.getExpressionIndexRecordsIterator(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m352\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.lambda$getExprIndexRecords$69dbdb2c$1(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m320\u001b[39m)",
      "  org.apache.hudi.data.HoodieJavaRDD.lambda$flatMap$a6598fcb$1(\u001b[32mHoodieJavaRDD.java\u001b[39m:\u001b[32m160\u001b[39m)",
      "  org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(\u001b[32mJavaRDDLike.scala\u001b[39m:\u001b[32m125\u001b[39m)",
      "  scala.collection.Iterator$$anon$11.nextCur(\u001b[32mIterator.scala\u001b[39m:\u001b[32m486\u001b[39m)",
      "  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m492\u001b[39m)",
      "  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)",
      "  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m760\u001b[39m)",
      "  org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(\u001b[32mSortAggregateExec.scala\u001b[39m:\u001b[32m62\u001b[39m)",
      "  org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(\u001b[32mSortAggregateExec.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m877\u001b[39m)",
      "  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(\u001b[32mRDD.scala\u001b[39m:\u001b[32m877\u001b[39m)",
      "  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m365\u001b[39m)",
      "  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m329\u001b[39m)",
      "  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m365\u001b[39m)",
      "  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m329\u001b[39m)",
      "  org.apache.spark.shuffle.ShuffleWriteProcessor.write(\u001b[32mShuffleWriteProcessor.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m136\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m548\u001b[39m)",
      "  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1504\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m551\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1128\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m628\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m829\u001b[39m)",
      "\u001b[31morg.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/hudi/hudi_indexed_table/san_francisco/057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-104-262_20250302225607026.parquet\u001b[39m",
      "  org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(\u001b[32mInternalParquetRecordReader.java\u001b[39m:\u001b[32m254\u001b[39m)",
      "  org.apache.parquet.hadoop.ParquetReader.read(\u001b[32mParquetReader.java\u001b[39m:\u001b[32m132\u001b[39m)",
      "  org.apache.parquet.hadoop.ParquetReader.read(\u001b[32mParquetReader.java\u001b[39m:\u001b[32m136\u001b[39m)",
      "  org.apache.hudi.common.util.ParquetReaderIterator.hasNext(\u001b[32mParquetReaderIterator.java\u001b[39m:\u001b[32m49\u001b[39m)",
      "  org.apache.hudi.common.util.collection.MappingIterator.hasNext(\u001b[32mMappingIterator.java\u001b[39m:\u001b[32m39\u001b[39m)",
      "  java.util.Iterator.forEachRemaining(\u001b[32mIterator.java\u001b[39m:\u001b[32m132\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.getBaseFileRecords(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m260\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.readRecordsAsRows(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m232\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.getExpressionIndexRecordsIterator(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m352\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.lambda$getExprIndexRecords$69dbdb2c$1(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m320\u001b[39m)",
      "  org.apache.hudi.data.HoodieJavaRDD.lambda$flatMap$a6598fcb$1(\u001b[32mHoodieJavaRDD.java\u001b[39m:\u001b[32m160\u001b[39m)",
      "  org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(\u001b[32mJavaRDDLike.scala\u001b[39m:\u001b[32m125\u001b[39m)",
      "  scala.collection.Iterator$$anon$11.nextCur(\u001b[32mIterator.scala\u001b[39m:\u001b[32m486\u001b[39m)",
      "  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m492\u001b[39m)",
      "  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)",
      "  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m760\u001b[39m)",
      "  org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(\u001b[32mSortAggregateExec.scala\u001b[39m:\u001b[32m62\u001b[39m)",
      "  org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(\u001b[32mSortAggregateExec.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m877\u001b[39m)",
      "  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(\u001b[32mRDD.scala\u001b[39m:\u001b[32m877\u001b[39m)",
      "  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m365\u001b[39m)",
      "  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m329\u001b[39m)",
      "  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m365\u001b[39m)",
      "  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m329\u001b[39m)",
      "  org.apache.spark.shuffle.ShuffleWriteProcessor.write(\u001b[32mShuffleWriteProcessor.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m136\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m548\u001b[39m)",
      "  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1504\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m551\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1128\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m628\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m829\u001b[39m)",
      "\u001b[31morg.apache.parquet.io.ParquetDecodingException: The requested schema is not compatible with the file schema. incompatible types: required int64 ts != optional int64 ts\u001b[39m",
      "  org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.incompatibleSchema(\u001b[32mColumnIOFactory.java\u001b[39m:\u001b[32m101\u001b[39m)",
      "  org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visitChildren(\u001b[32mColumnIOFactory.java\u001b[39m:\u001b[32m81\u001b[39m)",
      "  org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visit(\u001b[32mColumnIOFactory.java\u001b[39m:\u001b[32m57\u001b[39m)",
      "  org.apache.parquet.schema.MessageType.accept(\u001b[32mMessageType.java\u001b[39m:\u001b[32m55\u001b[39m)",
      "  org.apache.parquet.io.ColumnIOFactory.getColumnIO(\u001b[32mColumnIOFactory.java\u001b[39m:\u001b[32m162\u001b[39m)",
      "  org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(\u001b[32mInternalParquetRecordReader.java\u001b[39m:\u001b[32m135\u001b[39m)",
      "  org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(\u001b[32mInternalParquetRecordReader.java\u001b[39m:\u001b[32m225\u001b[39m)",
      "  org.apache.parquet.hadoop.ParquetReader.read(\u001b[32mParquetReader.java\u001b[39m:\u001b[32m132\u001b[39m)",
      "  org.apache.parquet.hadoop.ParquetReader.read(\u001b[32mParquetReader.java\u001b[39m:\u001b[32m136\u001b[39m)",
      "  org.apache.hudi.common.util.ParquetReaderIterator.hasNext(\u001b[32mParquetReaderIterator.java\u001b[39m:\u001b[32m49\u001b[39m)",
      "  org.apache.hudi.common.util.collection.MappingIterator.hasNext(\u001b[32mMappingIterator.java\u001b[39m:\u001b[32m39\u001b[39m)",
      "  java.util.Iterator.forEachRemaining(\u001b[32mIterator.java\u001b[39m:\u001b[32m132\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.getBaseFileRecords(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m260\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.readRecordsAsRows(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m232\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.getExpressionIndexRecordsIterator(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m352\u001b[39m)",
      "  org.apache.hudi.client.utils.SparkMetadataWriterUtils.lambda$getExprIndexRecords$69dbdb2c$1(\u001b[32mSparkMetadataWriterUtils.java\u001b[39m:\u001b[32m320\u001b[39m)",
      "  org.apache.hudi.data.HoodieJavaRDD.lambda$flatMap$a6598fcb$1(\u001b[32mHoodieJavaRDD.java\u001b[39m:\u001b[32m160\u001b[39m)",
      "  org.apache.spark.api.java.JavaRDDLike.$anonfun$flatMap$1(\u001b[32mJavaRDDLike.scala\u001b[39m:\u001b[32m125\u001b[39m)",
      "  scala.collection.Iterator$$anon$11.nextCur(\u001b[32mIterator.scala\u001b[39m:\u001b[32m486\u001b[39m)",
      "  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m492\u001b[39m)",
      "  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)",
      "  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)",
      "  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m760\u001b[39m)",
      "  org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(\u001b[32mSortAggregateExec.scala\u001b[39m:\u001b[32m62\u001b[39m)",
      "  org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(\u001b[32mSortAggregateExec.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m877\u001b[39m)",
      "  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(\u001b[32mRDD.scala\u001b[39m:\u001b[32m877\u001b[39m)",
      "  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m365\u001b[39m)",
      "  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m329\u001b[39m)",
      "  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m365\u001b[39m)",
      "  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m329\u001b[39m)",
      "  org.apache.spark.shuffle.ShuffleWriteProcessor.write(\u001b[32mShuffleWriteProcessor.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m99\u001b[39m)",
      "  org.apache.spark.scheduler.ShuffleMapTask.runTask(\u001b[32mShuffleMapTask.scala\u001b[39m:\u001b[32m52\u001b[39m)",
      "  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m136\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m548\u001b[39m)",
      "  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1504\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m551\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1128\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m628\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m829\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// Update a record and query the table based on indexed columns\n",
    "spark.sql(s\"SET hoodie.datasource.write.row.writer.enable=false\");\n",
    "spark.sql(s\"UPDATE hudi_indexed_table SET rider = 'rider-B', driver = 'driver-N', ts = '1697516137' WHERE rider = 'rider-A'\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|uuid|rider|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name| ts|uuid|rider|driver|fare|city|\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name| ts|uuid|rider|driver|fare|city|\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Data skipping would be performed using column stat expression index\n",
    "spark.sql(s\"SELECT uuid, rider FROM hudi_indexed_table WHERE from_unixtime(ts/1000, 'yyyy-MM-dd') = '2023-10-17'\").show();\n",
    "// Data skipping would be performed using bloom filter expression index\n",
    "spark.sql(s\"SELECT * FROM hudi_indexed_table WHERE driver = 'driver-N'\").show();\n",
    "// Data skipping would be performed using secondary index\n",
    "spark.sql(s\"SELECT * FROM hudi_indexed_table WHERE rider = 'rider-B'\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chennai\n",
      "san_francisco\n",
      "sao_paulo\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val result = \"ls /tmp/hudi/hudi_indexed_table\".!!\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 428K\n",
      "-rw-r--r-- 1 ryan ryan 426K Mar  2 22:52 b3974ad2-60a1-4f48-984f-bd0d60746d90-0_2-31-57_20250302225231787.parquet\n",
      "\n",
      "total 856K\n",
      "-rw-r--r-- 1 ryan ryan 427K Mar  2 22:53 93b32494-d7f7-4990-a8e3-1adad5fdc71a-0_0-104-262_20250302225321356.parquet\n",
      "-rw-r--r-- 1 ryan ryan 427K Mar  2 22:52 93b32494-d7f7-4990-a8e3-1adad5fdc71a-0_0-31-55_20250302225231787.parquet\n",
      "\n",
      "total 428K\n",
      "-rw-r--r-- 1 ryan ryan 426K Mar  2 22:52 4396ee58-28aa-4d71-8027-94867bfda52b-0_1-31-56_20250302225231787.parquet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result1</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result2</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result3</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mresult1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m\n",
       "\u001b[36mresult2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m\n",
       "\u001b[36mresult3\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val result1 = \"ls -lh /tmp/hudi/hudi_indexed_table/chennai\".!!\n",
    "println(result1)\n",
    "lazy val result2 = \"ls -lh /tmp/hudi/hudi_indexed_table/san_francisco\".!!\n",
    "println(result2)\n",
    "lazy val result3 = \"ls -lh /tmp/hudi/hudi_indexed_table/sao_paulo\".!!\n",
    "println(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+\n",
      "|          index_name|       col_name|index_type|\n",
      "+--------------------+---------------+----------+\n",
      "|        record_index|   record_index|          |\n",
      "|        column_stats|   column_stats|          |\n",
      "|     partition_stats|partition_stats|          |\n",
      "|secondary_index_i...|secondary_index|     rider|\n",
      "|expr_index_idx_co...|   column_stats|        ts|\n",
      "|expr_index_idx_bl...|  bloom_filters|    driver|\n",
      "+--------------------+---------------+----------+\n",
      "\n",
      "+---------------+---------------+----------+\n",
      "|     index_name|       col_name|index_type|\n",
      "+---------------+---------------+----------+\n",
      "|   column_stats|   column_stats|          |\n",
      "|partition_stats|partition_stats|          |\n",
      "+---------------+---------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres13_2\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres13_3\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres13_4\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres13_6\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: string, value: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop all the indexes\n",
    "spark.sql(s\"SHOW INDEXES FROM hudi_indexed_table\").show();\n",
    "spark.sql(s\"DROP INDEX secondary_index_idx_rider on hudi_indexed_table\");\n",
    "spark.sql(s\"DROP INDEX record_index on hudi_indexed_table\");\n",
    "spark.sql(s\"DROP INDEX expr_index_idx_bloom_driver on hudi_indexed_table\");\n",
    "spark.sql(s\"DROP INDEX expr_index_idx_column_ts on hudi_indexed_table\");\n",
    "// No indexes should show up for the table\n",
    "spark.sql(s\"SHOW INDEXES FROM hudi_indexed_table\").show();\n",
    "\n",
    "spark.sql(s\"SET hoodie.metadata.record.index.enable=false\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/hudi/hudi_indexed_table\n",
      "├── .hoodie\n",
      "│   ├── .aux\n",
      "│   │   └── .bootstrap\n",
      "│   │       ├── .fileids\n",
      "│   │       └── .partitions\n",
      "│   ├── .hoodie.properties.crc\n",
      "│   ├── .index_defs\n",
      "│   │   ├── .index.json.crc\n",
      "│   │   └── index.json\n",
      "│   ├── .schema\n",
      "│   ├── .temp\n",
      "│   │   └── 20250302225607026\n",
      "│   │       ├── .MARKERS.type.crc\n",
      "│   │       ├── .MARKERS0.crc\n",
      "│   │       ├── MARKERS.type\n",
      "│   │       └── MARKERS0\n",
      "│   ├── hoodie.properties\n",
      "│   ├── metadata\n",
      "│   │   ├── .hoodie\n",
      "│   │   │   ├── .aux\n",
      "│   │   │   │   └── .bootstrap\n",
      "│   │   │   │       ├── .fileids\n",
      "│   │   │   │       └── .partitions\n",
      "│   │   │   ├── .heartbeat\n",
      "│   │   │   ├── .hoodie.properties.crc\n",
      "│   │   │   ├── .schema\n",
      "│   │   │   ├── .temp\n",
      "│   │   │   ├── hoodie.properties\n",
      "│   │   │   └── timeline\n",
      "│   │   │       ├── .00000000000000000.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000000.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000000_20250302225556129.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000001.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000001_20250302225556334.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000002.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000002_20250302225556733.deltacommit.crc\n",
      "│   │   │       ├── .00000000000000003.deltacommit.inflight.crc\n",
      "│   │   │       ├── .00000000000000003.deltacommit.requested.crc\n",
      "│   │   │       ├── .00000000000000003_20250302225600414.deltacommit.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225554839.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225554839_20250302225558306.deltacommit.crc\n",
      "│   │   │       ├── .20250302225559340.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225559340.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225559340_20250302225604091.deltacommit.crc\n",
      "│   │   │       ├── .20250302225603213.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225603213.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225603213_20250302225605321.deltacommit.crc\n",
      "│   │   │       ├── .20250302225604816.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225604816.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225604816_20250302225606067.deltacommit.crc\n",
      "│   │   │       ├── .20250302225605606.deltacommit.inflight.crc\n",
      "│   │   │       ├── .20250302225605606.deltacommit.requested.crc\n",
      "│   │   │       ├── .20250302225605606_20250302225714131.deltacommit.crc\n",
      "│   │   │       ├── .20250302225713866.rollback.inflight.crc\n",
      "│   │   │       ├── .20250302225713866.rollback.requested.crc\n",
      "│   │   │       ├── .20250302225713866_20250302225714014.rollback.crc\n",
      "│   │   │       ├── 00000000000000000.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000000.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000000_20250302225556129.deltacommit\n",
      "│   │   │       ├── 00000000000000001.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000001.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000001_20250302225556334.deltacommit\n",
      "│   │   │       ├── 00000000000000002.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000002.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000002_20250302225556733.deltacommit\n",
      "│   │   │       ├── 00000000000000003.deltacommit.inflight\n",
      "│   │   │       ├── 00000000000000003.deltacommit.requested\n",
      "│   │   │       ├── 00000000000000003_20250302225600414.deltacommit\n",
      "│   │   │       ├── 20250302225554839.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225554839.deltacommit.requested\n",
      "│   │   │       ├── 20250302225554839_20250302225558306.deltacommit\n",
      "│   │   │       ├── 20250302225559340.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225559340.deltacommit.requested\n",
      "│   │   │       ├── 20250302225559340_20250302225604091.deltacommit\n",
      "│   │   │       ├── 20250302225603213.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225603213.deltacommit.requested\n",
      "│   │   │       ├── 20250302225603213_20250302225605321.deltacommit\n",
      "│   │   │       ├── 20250302225604816.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225604816.deltacommit.requested\n",
      "│   │   │       ├── 20250302225604816_20250302225606067.deltacommit\n",
      "│   │   │       ├── 20250302225605606.deltacommit.inflight\n",
      "│   │   │       ├── 20250302225605606.deltacommit.requested\n",
      "│   │   │       ├── 20250302225605606_20250302225714131.deltacommit\n",
      "│   │   │       ├── 20250302225713866.rollback.inflight\n",
      "│   │   │       ├── 20250302225713866.rollback.requested\n",
      "│   │   │       ├── 20250302225713866_20250302225714014.rollback\n",
      "│   │   │       └── history\n",
      "│   │   ├── column_stats\n",
      "│   │   │   ├── ..col-stats-0000-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0000-0_20250302225554839.log.1_0-42-85.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_00000000000000001.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..col-stats-0001-0_20250302225554839.log.1_1-42-86.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .col-stats-0000-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0000-0_20250302225554839.log.1_0-42-85\n",
      "│   │   │   ├── .col-stats-0001-0_00000000000000001.log.1_0-0-0\n",
      "│   │   │   ├── .col-stats-0001-0_20250302225554839.log.1_1-42-86\n",
      "│   │   │   └── .hoodie_partition_metadata\n",
      "│   │   ├── files\n",
      "│   │   │   ├── ..files-0000-0_00000000000000000.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..files-0000-0_20250302225554839.log.1_3-42-88.crc\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── .files-0000-0_0-5-4_00000000000000000.hfile.crc\n",
      "│   │   │   ├── .files-0000-0_00000000000000000.log.1_0-0-0\n",
      "│   │   │   ├── .files-0000-0_20250302225554839.log.1_3-42-88\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   └── files-0000-0_0-5-4_00000000000000000.hfile\n",
      "│   │   ├── partition_stats\n",
      "│   │   │   ├── ..hoodie_partition_metadata.crc\n",
      "│   │   │   ├── ..partition-stats-0000-0_00000000000000002.log.1_0-0-0.crc\n",
      "│   │   │   ├── ..partition-stats-0000-0_20250302225554839.log.1_2-42-87.crc\n",
      "│   │   │   ├── .hoodie_partition_metadata\n",
      "│   │   │   ├── .partition-stats-0000-0_00000000000000002.log.1_0-0-0\n",
      "│   │   │   └── .partition-stats-0000-0_20250302225554839.log.1_2-42-87\n",
      "│   │   └── record_index\n",
      "│   │       ├── ..hoodie_partition_metadata.crc\n",
      "│   │       ├── ..record-index-0000-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0001-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0002-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0003-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0004-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0005-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0006-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0007-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0008-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── ..record-index-0009-0_20250302225605606.log.1_0-0-0.crc\n",
      "│   │       ├── .hoodie_partition_metadata\n",
      "│   │       ├── .record-index-0000-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0001-0_1-129-354_20250302225605606.hfile.crc\n",
      "│   │       ├── .record-index-0001-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0002-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0003-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0003-0_3-129-355_20250302225605606.hfile.crc\n",
      "│   │       ├── .record-index-0004-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0004-0_4-129-356_20250302225605606.hfile.crc\n",
      "│   │       ├── .record-index-0005-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0006-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0007-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0007-0_7-129-357_20250302225605606.hfile.crc\n",
      "│   │       ├── .record-index-0008-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0009-0_20250302225605606.log.1_0-0-0\n",
      "│   │       ├── .record-index-0009-0_9-129-358_20250302225605606.hfile.crc\n",
      "│   │       ├── record-index-0001-0_1-129-354_20250302225605606.hfile\n",
      "│   │       ├── record-index-0003-0_3-129-355_20250302225605606.hfile\n",
      "│   │       ├── record-index-0004-0_4-129-356_20250302225605606.hfile\n",
      "│   │       ├── record-index-0007-0_7-129-357_20250302225605606.hfile\n",
      "│   │       └── record-index-0009-0_9-129-358_20250302225605606.hfile\n",
      "│   └── timeline\n",
      "│       ├── .20250302225554839.commit.requested.crc\n",
      "│       ├── .20250302225554839.inflight.crc\n",
      "│       ├── .20250302225554839_20250302225558333.commit.crc\n",
      "│       ├── .20250302225559340.indexing.inflight.crc\n",
      "│       ├── .20250302225559340.indexing.requested.crc\n",
      "│       ├── .20250302225559340_20250302225600498.indexing.crc\n",
      "│       ├── .20250302225603213.indexing.inflight.crc\n",
      "│       ├── .20250302225603213.indexing.requested.crc\n",
      "│       ├── .20250302225603213_20250302225604159.indexing.crc\n",
      "│       ├── .20250302225604816.indexing.inflight.crc\n",
      "│       ├── .20250302225604816.indexing.requested.crc\n",
      "│       ├── .20250302225604816_20250302225605377.indexing.crc\n",
      "│       ├── .20250302225605606.indexing.inflight.crc\n",
      "│       ├── .20250302225605606.indexing.requested.crc\n",
      "│       ├── .20250302225605606_20250302225606123.indexing.crc\n",
      "│       ├── .20250302225607026.commit.requested.crc\n",
      "│       ├── .20250302225607026.inflight.crc\n",
      "│       ├── 20250302225554839.commit.requested\n",
      "│       ├── 20250302225554839.inflight\n",
      "│       ├── 20250302225554839_20250302225558333.commit\n",
      "│       ├── 20250302225559340.indexing.inflight\n",
      "│       ├── 20250302225559340.indexing.requested\n",
      "│       ├── 20250302225559340_20250302225600498.indexing\n",
      "│       ├── 20250302225603213.indexing.inflight\n",
      "│       ├── 20250302225603213.indexing.requested\n",
      "│       ├── 20250302225603213_20250302225604159.indexing\n",
      "│       ├── 20250302225604816.indexing.inflight\n",
      "│       ├── 20250302225604816.indexing.requested\n",
      "│       ├── 20250302225604816_20250302225605377.indexing\n",
      "│       ├── 20250302225605606.indexing.inflight\n",
      "│       ├── 20250302225605606.indexing.requested\n",
      "│       ├── 20250302225605606_20250302225606123.indexing\n",
      "│       ├── 20250302225607026.commit.requested\n",
      "│       ├── 20250302225607026.inflight\n",
      "│       └── history\n",
      "├── chennai\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   └── 56d56bfc-255f-49d9-9636-6b5e9d40239c-0_2-31-57_20250302225554839.parquet\n",
      "├── san_francisco\n",
      "│   ├── ..hoodie_partition_metadata.crc\n",
      "│   ├── .057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-104-262_20250302225607026.parquet.crc\n",
      "│   ├── .057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet.crc\n",
      "│   ├── .hoodie_partition_metadata\n",
      "│   ├── 057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-104-262_20250302225607026.parquet\n",
      "│   └── 057fe24d-fb09-4522-b25a-0006c6e0508f-0_0-31-55_20250302225554839.parquet\n",
      "└── sao_paulo\n",
      "    ├── ..hoodie_partition_metadata.crc\n",
      "    ├── .e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet.crc\n",
      "    ├── .hoodie_partition_metadata\n",
      "    └── e5fab59c-37cd-4243-851a-06841a3ac3c6-0_1-31-56_20250302225554839.parquet\n",
      "\n",
      "29 directories, 174 files\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val result = \"tree -a /tmp/hudi/hudi_indexed_table\".!!\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|           ts|                uuid|  rider|  driver| fare|         city|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "|  20250302225554839|20250302225554839...|334e26e9-8355-45c...|         san_francisco|057fe24d-fb09-452...|1695159649087|334e26e9-8355-45c...|rider-A|driver-K| 19.1|san_francisco|\n",
      "|  20250302225554839|20250302225554839...|9909a8b1-2d15-4d3...|         san_francisco|057fe24d-fb09-452...|1695046462179|9909a8b1-2d15-4d3...|rider-D|driver-L| 33.9|san_francisco|\n",
      "|  20250302225554839|20250302225554839...|e96c4396-3fad-413...|         san_francisco|057fe24d-fb09-452...|1695091554788|e96c4396-3fad-413...|rider-C|driver-M| 27.7|san_francisco|\n",
      "|  20250302225554839|20250302225554839...|e3cf430c-889d-401...|             sao_paulo|e5fab59c-37cd-424...|1695516137016|e3cf430c-889d-401...|rider-F|driver-P|34.15|    sao_paulo|\n",
      "|  20250302225554839|20250302225554839...|c8abbe79-8d89-47e...|               chennai|56d56bfc-255f-49d...|1695115999911|c8abbe79-8d89-47e...|rider-J|driver-T|17.85|      chennai|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|           ts|                uuid|  rider|  driver| fare|         city|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "|  20250302225554839|20250302225554839...|334e26e9-8355-45c...|         san_francisco|057fe24d-fb09-452...|1695159649087|334e26e9-8355-45c...|rider-A|driver-K| 19.1|san_francisco|\n",
      "|  20250302225554839|20250302225554839...|9909a8b1-2d15-4d3...|         san_francisco|057fe24d-fb09-452...|1695046462179|9909a8b1-2d15-4d3...|rider-D|driver-L| 33.9|san_francisco|\n",
      "|  20250302225554839|20250302225554839...|e96c4396-3fad-413...|         san_francisco|057fe24d-fb09-452...|1695091554788|e96c4396-3fad-413...|rider-C|driver-M| 27.7|san_francisco|\n",
      "|  20250302225554839|20250302225554839...|e3cf430c-889d-401...|             sao_paulo|e5fab59c-37cd-424...|1695516137016|e3cf430c-889d-401...|rider-F|driver-P|34.15|    sao_paulo|\n",
      "|  20250302225554839|20250302225554839...|c8abbe79-8d89-47e...|               chennai|56d56bfc-255f-49d...|1695115999911|c8abbe79-8d89-47e...|rider-J|driver-T|17.85|      chennai|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name| ts|uuid|rider|driver|fare|city|\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+---+----+-----+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"hudi\").\n",
    "  option(\"as.of.instant\", \"20250302225605606\").\n",
    "  load(basePath).show()\n",
    "\n",
    "spark.read.format(\"hudi\").\n",
    "  option(\"as.of.instant\", \"2025-03-02 22:56:01.200\").\n",
    "  load(basePath).show()\n",
    "\n",
    "// It is equal to \"as.of.instant = 2021-07-28 00:00:00\"\n",
    "spark.read.format(\"hudi\").\n",
    "  option(\"as.of.instant\", \"2025-03-02\").\n",
    "  load(basePath).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
