{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/hudi/\n",
      "\n",
      "0 directories, 0 files\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">import </span></span><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">sys.process._</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">res1_1</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">res1_2</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">result3</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\u001b[39m\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\u001b[39m\n",
       "\u001b[36mres1_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\u001b[39m\n",
       "\u001b[36mresult3\u001b[39m: \u001b[32mString\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"rm -rf /tmp/hudi/\".!!\n",
    "\"mkdir -p /tmp/hudi\".!!\n",
    "lazy val result3 = \"tree -a /tmp/hudi/\".!!\n",
    "println(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/04 00:17:00 WARN Utils: Your hostname, DESKTOP-M94RUSC resolves to a loopback address: 127.0.1.1; using 172.17.75.227 instead (on interface eth0)\n",
      "25/03/04 00:17:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/04 00:17:01 INFO SparkContext: Running Spark version 3.3.2\n",
      "25/03/04 00:17:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/04 00:17:01 INFO ResourceUtils: ==============================================================\n",
      "25/03/04 00:17:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/04 00:17:01 INFO ResourceUtils: ==============================================================\n",
      "25/03/04 00:17:01 INFO SparkContext: Submitted application: HudiLocalSession\n",
      "25/03/04 00:17:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/04 00:17:01 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/03/04 00:17:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/04 00:17:01 INFO SecurityManager: Changing view acls to: ryan\n",
      "25/03/04 00:17:01 INFO SecurityManager: Changing modify acls to: ryan\n",
      "25/03/04 00:17:01 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/04 00:17:01 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/04 00:17:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ryan); groups with view permissions: Set(); users  with modify permissions: Set(ryan); groups with modify permissions: Set()\n",
      "25/03/04 00:17:02 INFO Utils: Successfully started service 'sparkDriver' on port 36063.\n",
      "25/03/04 00:17:02 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/04 00:17:02 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/04 00:17:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/04 00:17:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/04 00:17:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/04 00:17:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-46265b36-dc67-4b2d-9b5a-4ff642ea5ffd\n",
      "25/03/04 00:17:02 INFO MemoryStore: MemoryStore started with capacity 4.4 GiB\n",
      "25/03/04 00:17:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/04 00:17:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/04 00:17:02 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/03/04 00:17:02 INFO Executor: Starting executor ID driver on host 172.17.75.227\n",
      "25/03/04 00:17:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/03/04 00:17:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39779.\n",
      "25/03/04 00:17:02 INFO NettyBlockTransferService: Server created on 172.17.75.227:39779\n",
      "25/03/04 00:17:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/04 00:17:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.75.227, 39779, None)\n",
      "25/03/04 00:17:02 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.75.227:39779 with 4.4 GiB RAM, BlockManagerId(driver, 172.17.75.227, 39779, None)\n",
      "25/03/04 00:17:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.75.227, 39779, None)\n",
      "25/03/04 00:17:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.17.75.227, 39779, None)\n",
      "Spark with Hudi is ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$cp.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.QuickstartUtils._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.hive.HiveExternalCatalog\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConversions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.DataSourceReadOptions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.DataSourceWriteOptions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.common.table.HoodieTableConfig._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.config.HoodieWriteConfig._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.keygen.constant.KeyGeneratorOptions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.hudi.common.model.HoodieRecord\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.payloads.CustomMergeIntoConnector\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6dd4ec64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.hudi:hudi-spark3.3-bundle_2.12:0.14.0`\n",
    "// import $ivy.`org.apache.hudi:hudi-common:1.0.0`\n",
    "import $ivy.`org.apache.spark:spark-sql_2.12:3.3.2`\n",
    "import $ivy.`org.apache.spark:spark-avro_2.12:3.3.2`\n",
    "import $ivy.`org.apache.spark:spark-hive_2.12:3.3.2`\n",
    "import $cp.`CustomMergeIntoConnector.jar`\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hudi.QuickstartUtils._\n",
    "import org.apache.spark.sql.hive.HiveExternalCatalog\n",
    "import scala.collection.JavaConversions._\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.hudi.DataSourceReadOptions._\n",
    "import org.apache.hudi.DataSourceWriteOptions._\n",
    "import org.apache.hudi.common.table.HoodieTableConfig._\n",
    "import org.apache.hudi.config.HoodieWriteConfig._\n",
    "import org.apache.hudi.keygen.constant.KeyGeneratorOptions._\n",
    "import org.apache.hudi.common.model.HoodieRecord\n",
    "import com.payloads.CustomMergeIntoConnector\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"HudiLocalSession\")\n",
    "  .master(\"local[1]\")  // Runs on local machine with 1 local worker / thread, so we can see the impact on COPY_ON_WRITE\n",
    "  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "  .config(\"spark.kryo.registrator\", \"org.apache.spark.HoodieSparkKryoRegistrar\")\n",
    "  .config(\"spark.driver.extraJavaOptions\", \"-Dscala.repl.maxprintstring=0\")\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "  \n",
    "// http://localhost:4040/jobs/\n",
    "println(\"Spark with Hudi is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Insert Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n",
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">columns</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Seq</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>] = <span style=\"color: yellow\"><span class=\"ansi-yellow-fg\">List</span></span>(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;ts&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;uuid&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;fare&quot;</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;city&quot;</span></span>)\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">data</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Seq</span></span>[(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Long</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>)] = <span style=\"color: yellow\"><span class=\"ansi-yellow-fg\">List</span></span>(\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695159649087L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;334e26e9-8355-45cc-97c6-c31daf0df330&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-A&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-K&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">19.1</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;san_francisco&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695091554788L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;e96c4396-3fad-413a-a942-4cb36106d721&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-C&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-M&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">27.7</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;san_francisco&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695046462179L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;9909a8b1-2d15-4d3d-8ec9-efc48c536a00&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-D&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-L&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">33.9</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;san_francisco&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695516137016L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;e3cf430c-889d-4015-bc98-59bdce1e530c&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-F&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-P&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">34.15</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;sao_paulo&quot;</span></span>\n",
       "  ),\n",
       "  (\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1695115999911L</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;c8abbe79-8d89-47ea-b4ce-4d224bae5bfa&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;rider-J&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;driver-T&quot;</span></span>,\n",
       "    <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">17.85</span></span>,\n",
       "...\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">tableName</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;trips_table&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">basePath</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;file:///tmp/hudi/trips_table&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">inserts</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [ts: bigint, uuid: string ... 4 more fields]</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mcolumns\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"ts\"\u001b[39m, \u001b[32m\"uuid\"\u001b[39m, \u001b[32m\"rider\"\u001b[39m, \u001b[32m\"driver\"\u001b[39m, \u001b[32m\"fare\"\u001b[39m, \u001b[32m\"city\"\u001b[39m)\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\n",
       "    \u001b[32m1695159649087L\u001b[39m,\n",
       "    \u001b[32m\"334e26e9-8355-45cc-97c6-c31daf0df330\"\u001b[39m,\n",
       "    \u001b[32m\"rider-A\"\u001b[39m,\n",
       "    \u001b[32m\"driver-K\"\u001b[39m,\n",
       "    \u001b[32m19.1\u001b[39m,\n",
       "    \u001b[32m\"san_francisco\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695091554788L\u001b[39m,\n",
       "    \u001b[32m\"e96c4396-3fad-413a-a942-4cb36106d721\"\u001b[39m,\n",
       "    \u001b[32m\"rider-C\"\u001b[39m,\n",
       "    \u001b[32m\"driver-M\"\u001b[39m,\n",
       "    \u001b[32m27.7\u001b[39m,\n",
       "    \u001b[32m\"san_francisco\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695046462179L\u001b[39m,\n",
       "    \u001b[32m\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\"\u001b[39m,\n",
       "    \u001b[32m\"rider-D\"\u001b[39m,\n",
       "    \u001b[32m\"driver-L\"\u001b[39m,\n",
       "    \u001b[32m33.9\u001b[39m,\n",
       "    \u001b[32m\"san_francisco\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695516137016L\u001b[39m,\n",
       "    \u001b[32m\"e3cf430c-889d-4015-bc98-59bdce1e530c\"\u001b[39m,\n",
       "    \u001b[32m\"rider-F\"\u001b[39m,\n",
       "    \u001b[32m\"driver-P\"\u001b[39m,\n",
       "    \u001b[32m34.15\u001b[39m,\n",
       "    \u001b[32m\"sao_paulo\"\u001b[39m\n",
       "  ),\n",
       "  (\n",
       "    \u001b[32m1695115999911L\u001b[39m,\n",
       "    \u001b[32m\"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa\"\u001b[39m,\n",
       "    \u001b[32m\"rider-J\"\u001b[39m,\n",
       "    \u001b[32m\"driver-T\"\u001b[39m,\n",
       "    \u001b[32m17.85\u001b[39m,\n",
       "...\n",
       "\u001b[36mtableName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"trips_table\"\u001b[39m\n",
       "\u001b[36mbasePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"file:///tmp/hudi/trips_table\"\u001b[39m\n",
       "\u001b[36minserts\u001b[39m: \u001b[32mDataFrame\u001b[39m = [ts: bigint, uuid: string ... 4 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val columns = Seq(\"ts\",\"uuid\",\"rider\",\"driver\",\"fare\",\"city\")\n",
    "val data =\n",
    "  Seq((1695159649087L,\"334e26e9-8355-45cc-97c6-c31daf0df330\",\"rider-A\",\"driver-K\",19.10,\"san_francisco\"),\n",
    "    (1695091554788L,\"e96c4396-3fad-413a-a942-4cb36106d721\",\"rider-C\",\"driver-M\",27.70 ,\"san_francisco\"),\n",
    "    (1695046462179L,\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\",\"rider-D\",\"driver-L\",33.90 ,\"san_francisco\"),\n",
    "    (1695516137016L,\"e3cf430c-889d-4015-bc98-59bdce1e530c\",\"rider-F\",\"driver-P\",34.15,\"sao_paulo\"    ),\n",
    "    (1695115999911L,\"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa\",\"rider-J\",\"driver-T\",17.85,\"chennai\"));\n",
    "\n",
    "val tableName = \"trips_table\"\n",
    "val basePath = \"file:///tmp/hudi/trips_table\"\n",
    "var inserts = spark.createDataFrame(data).toDF(columns:_*)\n",
    "\n",
    "inserts.write.format(\"hudi\").\n",
    "  option(\"hoodie.datasource.write.partitionpath.field\", \"city\").\n",
    "  option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\"). // Force hive style dir naming for partitions.\n",
    "  option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\").\n",
    "  option(\"hoodie.table.name\", tableName).\n",
    "  mode(Overwrite).\n",
    "  save(basePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we check the Hudi table dir, we can find several parquet files.\n",
    "* Here we have 1 parquet file for each city/partition.\n",
    "* This is because we set the number of worker to 1 and the file is tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mupdatesDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 9 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val updatesDf = spark.read.format(\"hudi\").load(basePath).filter($\"rider\" === \"rider-D\").withColumn(\"fare\", col(\"fare\") * 10)\n",
    "\n",
    "updatesDf.write.format(\"hudi\").\n",
    "  option(\"hoodie.datasource.write.operation\", \"upsert\").\n",
    "  option(\"hoodie.datasource.write.partitionpath.field\", \"city\").\n",
    "  option(\"hoodie.table.name\", tableName).\n",
    "  mode(Append).\n",
    "  save(basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|           ts|                uuid|  rider|  driver| fare|         city|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "|  20250304001809024|20250304001809024...|20250304001707251...|    city=san_francisco|14e71d60-c51e-443...|1695159649087|334e26e9-8355-45c...|rider-A|driver-K|210.1|san_francisco|\n",
      "|  20250304001809024|20250304001809024...|20250304001707251...|    city=san_francisco|14e71d60-c51e-443...|1695091554788|e96c4396-3fad-413...|rider-C|driver-M|304.7|san_francisco|\n",
      "|  20250304001754658|20250304001754658...|20250304001707251...|    city=san_francisco|14e71d60-c51e-443...|1695046462179|9909a8b1-2d15-4d3...|rider-D|driver-L|339.0|san_francisco|\n",
      "|  20250304001707251|20250304001707251...|20250304001707251...|        city=sao_paulo|14e71d60-c51e-443...|1695516137016|e3cf430c-889d-401...|rider-F|driver-P|34.15|    sao_paulo|\n",
      "|  20250304001707251|20250304001707251...|20250304001707251...|          city=chennai|14e71d60-c51e-443...|1695115999911|c8abbe79-8d89-47e...|rider-J|driver-T|17.85|      chennai|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------------+--------------------+-------+--------+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36madjustedFareDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 9 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val adjustedFareDF = spark.read.format(\"hudi\").\n",
    "  load(basePath).limit(2).\n",
    "  withColumn(\"fare\", col(\"fare\") * 10)\n",
    "\n",
    "adjustedFareDF.write.format(\"hudi\").\n",
    "  option(\"hoodie.datasource.write.payload.class\",\"com.payloads.CustomMergeIntoConnector\").\n",
    "  mode(Append).\n",
    "  save(basePath)\n",
    "// Notice Fare column has been updated but all other columns remain intact.\n",
    "spark.read.format(\"hudi\").load(basePath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdeletesDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 9 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Lets  delete rider: rider-D\n",
    "val deletesDF = spark.read.format(\"hudi\").load(basePath).filter($\"rider\" === \"rider-F\")\n",
    "\n",
    "deletesDF.write.format(\"hudi\").\n",
    "  option(\"hoodie.datasource.write.operation\", \"delete\").\n",
    "  option(\"hoodie.datasource.write.partitionpath.field\", \"city\").\n",
    "  option(\"hoodie.table.name\", tableName).\n",
    "  mode(Append).\n",
    "  save(basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
